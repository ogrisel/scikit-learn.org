

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>3.2. Support Vector Machines &mdash; scikit-learn 0.10 documentation</title>
    
    <link rel="stylesheet" href="../_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '0.10',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="author" title="About these documents" href="../about.html" />
    <link rel="top" title="scikit-learn 0.10 documentation" href="../index.html" />
    <link rel="up" title="3. Supervised learning" href="../supervised_learning.html" />
    <link rel="next" title="3.3. Stochastic Gradient Descent" href="sgd.html" />
    <link rel="prev" title="3.1. Generalized Linear Models" href="linear_model.html" />
<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-22606712-2']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>

  </head>
  <body>
    <div class="header-wrapper">
      <div class="header">
          <p class="logo"><a href="../index.html">
            <img src="../_static/scikit-learn-logo-small.png" alt="Logo"/>
          </a>
          </p><div class="navbar">
          <ul>
            <li><a href="../install.html">Download</a></li>
            <li><a href="../support.html">Support</a></li>
            <li><a href="../user_guide.html">User Guide</a></li>
            <li><a href="../auto_examples/index.html">Examples</a></li>
            <li><a href="classes.html">Reference</a></li>
       </ul>

<div class="search_form">

<div id="cse" style="width: 100%;"></div>
<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">
  google.load('search', '1', {language : 'en'});
  google.setOnLoadCallback(function() {
    var customSearchControl = new google.search.CustomSearchControl('016639176250731907682:tjtqbvtvij0');
    customSearchControl.setResultSetSize(google.search.Search.FILTERED_CSE_RESULTSET);
    var options = new google.search.DrawOptions();
    options.setAutoComplete(true);
    customSearchControl.draw('cse', options);
  }, true);
</script>

</div>
          </div> <!-- end navbar --></div>
    </div>

    <div class="content-wrapper">

    <div class="sphinxsidebar">
	<div class="rel">
	
	<!-- rellinks[1:] is an ugly hack to avoid link to module
	    index  -->
	<div class="rellink">
	<a href="linear_model.html" title="3.1. Generalized Linear Models"
	    accesskey="P">Previous
	    <br>
	    <span class="smallrellink">
	    3.1. Generalized...
	    </span>
	    <span class="hiddenrellink">
	    3.1. Generalized Linear Models
	    </span>
	    
	    </a>
	</div>
	    <div class="spacer">
	    &nbsp;
	    </div>
	
	<div class="rellink">
	<a href="sgd.html" title="3.3. Stochastic Gradient Descent"
	    accesskey="N">Next
	    <br>
	    <span class="smallrellink">
	    3.3. Stochastic ...
	    </span>
	    <span class="hiddenrellink">
	    3.3. Stochastic Gradient Descent
	    </span>
	    
	    </a>
	</div>
	<!-- Ad a link to the 'up' page -->
	<div class="spacer">
	&nbsp;
	</div>
	<div class="rellink">
	<a href="../supervised_learning.html" title="3. Supervised learning" >
	Up
	<br>
	<span class="smallrellink">
	3. Supervised le...
	</span>
	<span class="hiddenrellink">
	3. Supervised learning
	</span>
	
	</a>
	</div>
    </div>
    <p style="text-align: center">This documentation is
    for scikit-learn <strong>version 0.10</strong>
    &mdash; <a href="http://scikit-learn.org/stable/support.html#documentation-resources">Other versions</a></p>
    
    <h3>Citing</h3>
    <p>If you use the software, please consider
    <a href="about.html#citing-scikit-learn">citing scikit-learn</a>.</p>
    <h3>This page</h3>
	<ul>
<li><a class="reference internal" href="#">3.2. Support Vector Machines</a><ul>
<li><a class="reference internal" href="#classification">3.2.1. Classification</a><ul>
<li><a class="reference internal" href="#multi-class-classification">3.2.1.1. Multi-class classification</a></li>
<li><a class="reference internal" href="#unbalanced-problems">3.2.1.2. Unbalanced problems</a></li>
</ul>
</li>
<li><a class="reference internal" href="#regression">3.2.2. Regression</a></li>
<li><a class="reference internal" href="#density-estimation-novelty-detection">3.2.3. Density estimation, novelty detection</a></li>
<li><a class="reference internal" href="#support-vector-machines-for-sparse-data">3.2.4. Support Vector machines for sparse data</a></li>
<li><a class="reference internal" href="#complexity">3.2.5. Complexity</a></li>
<li><a class="reference internal" href="#tips-on-practical-use">3.2.6. Tips on Practical Use</a></li>
<li><a class="reference internal" href="#kernel-functions">3.2.7. Kernel functions</a><ul>
<li><a class="reference internal" href="#custom-kernels">3.2.7.1. Custom Kernels</a><ul>
<li><a class="reference internal" href="#using-python-functions-as-kernels">3.2.7.1.1. Using python functions as kernels</a></li>
<li><a class="reference internal" href="#using-the-gram-matrix">3.2.7.1.2. Using the Gram matrix</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#mathematical-formulation">3.2.8. Mathematical formulation</a><ul>
<li><a class="reference internal" href="#svc">3.2.8.1. SVC</a></li>
<li><a class="reference internal" href="#nusvc">3.2.8.2. NuSVC</a></li>
</ul>
</li>
<li><a class="reference internal" href="#implementation-details">3.2.9. Implementation details</a></li>
</ul>
</li>
</ul>

    
    </div>

      <div class="content">
            
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="support-vector-machines">
<span id="svm"></span><h1>3.2. Support Vector Machines<a class="headerlink" href="#support-vector-machines" title="Permalink to this headline">¶</a></h1>
<p><strong>Support vector machines (SVMs)</strong> are a set of supervised learning
methods used for <a class="reference internal" href="#svm-classification"><em>classification</em></a>,
<a class="reference internal" href="#svm-regression"><em>regression</em></a> and <a class="reference internal" href="#svm-outlier-detection"><em>outliers detection</em></a>.</p>
<p>The advantages of Support Vector Machines are:</p>
<blockquote>
<div><ul class="simple">
<li>Effective in high dimensional spaces.</li>
<li>Still effective in cases where number of dimensions is greater
than the number of samples.</li>
<li>Uses a subset of training points in the decision function (called
support vectors), so it is also memory efficient.</li>
<li>Versatile: different <a class="reference internal" href="#svm-kernels"><em>Kernel functions</em></a> can be
specified for the decision function. Common kernels are
provided, but it is also possible to specify custom kernels.</li>
</ul>
</div></blockquote>
<p>The disadvantages of Support Vector Machines include:</p>
<blockquote>
<div><ul class="simple">
<li>If the number of features is much greater than the number of
samples, the method is likely to give poor performances.</li>
<li>SVMs do not directly provide probability estimates, these are
calculated using five-fold cross-validation, and thus
performance can suffer.</li>
</ul>
</div></blockquote>
<div class="section" id="classification">
<span id="svm-classification"></span><h2>3.2.1. Classification<a class="headerlink" href="#classification" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><tt class="xref py py-class docutils literal"><span class="pre">SVC</span></tt></a>, <a class="reference internal" href="generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC" title="sklearn.svm.NuSVC"><tt class="xref py py-class docutils literal"><span class="pre">NuSVC</span></tt></a> and <a class="reference internal" href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><tt class="xref py py-class docutils literal"><span class="pre">LinearSVC</span></tt></a> are classes
capable of performing multi-class classification on a dataset.</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/svm/plot_iris.html"><img alt="../_images/plot_iris_12.png" src="../_images/plot_iris_12.png" /></a>
</div>
<p><a class="reference internal" href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><tt class="xref py py-class docutils literal"><span class="pre">SVC</span></tt></a> and <a class="reference internal" href="generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC" title="sklearn.svm.NuSVC"><tt class="xref py py-class docutils literal"><span class="pre">NuSVC</span></tt></a> are similar methods, but accept
slightly different sets of parameters and have different mathematical
formulations (see section <a class="reference internal" href="#svm-mathematical-formulation"><em>Mathematical formulation</em></a>). On the
other hand, <a class="reference internal" href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><tt class="xref py py-class docutils literal"><span class="pre">LinearSVC</span></tt></a> is another implementation of Support
Vector Classification for the case of a linear kernel. Note that
<a class="reference internal" href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><tt class="xref py py-class docutils literal"><span class="pre">LinearSVC</span></tt></a> does not accept keyword &#8216;kernel&#8217;, as this is
assumed to be linear. It also lacks some of the members of
<a class="reference internal" href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><tt class="xref py py-class docutils literal"><span class="pre">SVC</span></tt></a> and <a class="reference internal" href="generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC" title="sklearn.svm.NuSVC"><tt class="xref py py-class docutils literal"><span class="pre">NuSVC</span></tt></a>, like support_.</p>
<p>As other classifiers, <a class="reference internal" href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><tt class="xref py py-class docutils literal"><span class="pre">SVC</span></tt></a>, <a class="reference internal" href="generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC" title="sklearn.svm.NuSVC"><tt class="xref py py-class docutils literal"><span class="pre">NuSVC</span></tt></a> and
<a class="reference internal" href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><tt class="xref py py-class docutils literal"><span class="pre">LinearSVC</span></tt></a> take as input two arrays: an array X of size
[n_samples, n_features] holding the training samples, and an array Y
of integer values, size [n_samples], holding the class labels for the
training samples:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">svm</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="go">SVC(C=1.0, cache_size=200, coef0=0.0, degree=3, gamma=0.5, kernel=&#39;rbf&#39;,</span>
<span class="go">  probability=False, scale_C=False, shrinking=True, tol=0.001)</span>
</pre></div>
</div>
<p>After being fitted, the model can then be used to predict new values:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">]])</span>
<span class="go">array([ 1.])</span>
</pre></div>
</div>
<p>SVMs decision function depends on some subset of the training data,
called the support vectors. Some properties of these support vectors
can be found in members <cite>support_vectors_</cite>, <cite>support_</cite> and
<cite>n_support</cite>:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="c"># get support vectors</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">support_vectors_</span>
<span class="go">array([[ 0.,  0.],</span>
<span class="go">       [ 1.,  1.]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c"># get indices of support vectors</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">support_</span> 
<span class="go">array([0, 1]...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c"># get number of support vectors for each class</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">n_support_</span> 
<span class="go">array([1, 1]...)</span>
</pre></div>
</div>
<div class="section" id="multi-class-classification">
<h3>3.2.1.1. Multi-class classification<a class="headerlink" href="#multi-class-classification" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><tt class="xref py py-class docutils literal"><span class="pre">SVC</span></tt></a> and <a class="reference internal" href="generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC" title="sklearn.svm.NuSVC"><tt class="xref py py-class docutils literal"><span class="pre">NuSVC</span></tt></a> implement the &#8220;one-against-one&#8221;
approach (Knerr et al., 1990) for multi- class classification. If
n_class is the number of classes, then n_class * (n_class - 1)/2
classifiers are constructed and each one trains data from two classes:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="go">SVC(C=1.0, cache_size=200, coef0=0.0, degree=3, gamma=1.0, kernel=&#39;rbf&#39;,</span>
<span class="go">  probability=False, scale_C=False, shrinking=True, tol=0.001)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dec</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">decision_function</span><span class="p">([[</span><span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dec</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="c"># 4 classes: 4*3/2 = 6</span>
<span class="go">6</span>
</pre></div>
</div>
<p>On the other hand, <a class="reference internal" href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><tt class="xref py py-class docutils literal"><span class="pre">LinearSVC</span></tt></a> implements &#8220;one-vs-the-rest&#8221;
multi-class strategy, thus training n_class models. If there are only
two classes, only one model is trained:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">lin_clf</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">LinearSVC</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lin_clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="go">LinearSVC(C=1.0, dual=True, fit_intercept=True, intercept_scaling=1,</span>
<span class="go">     loss=&#39;l2&#39;, multi_class=False, penalty=&#39;l2&#39;, scale_C=False, tol=0.0001)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dec</span> <span class="o">=</span> <span class="n">lin_clf</span><span class="o">.</span><span class="n">decision_function</span><span class="p">([[</span><span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dec</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="go">4</span>
</pre></div>
</div>
<p>See <a class="reference internal" href="#svm-mathematical-formulation"><em>Mathematical formulation</em></a> for a complete description of
the decision function.</p>
</div>
<div class="section" id="unbalanced-problems">
<h3>3.2.1.2. Unbalanced problems<a class="headerlink" href="#unbalanced-problems" title="Permalink to this headline">¶</a></h3>
<p>In problems where it is desired to give more importance to certain
classes or certain individual samples keywords <tt class="docutils literal"><span class="pre">class_weight</span></tt> and
<tt class="docutils literal"><span class="pre">sample_weight</span></tt> can be used.</p>
<p><a class="reference internal" href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><tt class="xref py py-class docutils literal"><span class="pre">SVC</span></tt></a> (but not <a class="reference internal" href="generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC" title="sklearn.svm.NuSVC"><tt class="xref py py-class docutils literal"><span class="pre">NuSVC</span></tt></a>) implement a keyword
<tt class="docutils literal"><span class="pre">class_weight</span></tt> in the fit method. It&#8217;s a dictionary of the form
<tt class="docutils literal"><span class="pre">{class_label</span> <span class="pre">:</span> <span class="pre">value}</span></tt>, where value is a floating point number &gt; 0
that sets the parameter C of class <tt class="docutils literal"><span class="pre">class_label</span></tt> to C * value.</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/svm/plot_separating_hyperplane_unbalanced.html"><img alt="../_images/plot_separating_hyperplane_unbalanced_11.png" src="../_images/plot_separating_hyperplane_unbalanced_11.png" style="width: 600.0px; height: 450.0px;" /></a>
</div>
<p><a class="reference internal" href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><tt class="xref py py-class docutils literal"><span class="pre">SVC</span></tt></a>, <a class="reference internal" href="generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC" title="sklearn.svm.NuSVC"><tt class="xref py py-class docutils literal"><span class="pre">NuSVC</span></tt></a>, <a class="reference internal" href="generated/sklearn.svm.SVR.html#sklearn.svm.SVR" title="sklearn.svm.SVR"><tt class="xref py py-class docutils literal"><span class="pre">SVR</span></tt></a>, <a class="reference internal" href="generated/sklearn.svm.NuSVR.html#sklearn.svm.NuSVR" title="sklearn.svm.NuSVR"><tt class="xref py py-class docutils literal"><span class="pre">NuSVR</span></tt></a> and
<a class="reference internal" href="generated/sklearn.svm.OneClassSVM.html#sklearn.svm.OneClassSVM" title="sklearn.svm.OneClassSVM"><tt class="xref py py-class docutils literal"><span class="pre">OneClassSVM</span></tt></a> implement also weights for individual samples in method
<tt class="docutils literal"><span class="pre">fit</span></tt> through keyword sample_weight.</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/svm/plot_weighted_samples.html"><img alt="../_images/plot_weighted_samples_11.png" src="../_images/plot_weighted_samples_11.png" style="width: 600.0px; height: 450.0px;" /></a>
</div>
<div class="topic">
<p class="topic-title first">Examples:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/svm/plot_iris.html#example-svm-plot-iris-py"><em>Plot different SVM classifiers in the iris dataset</em></a>,</li>
<li><a class="reference internal" href="../auto_examples/svm/plot_separating_hyperplane.html#example-svm-plot-separating-hyperplane-py"><em>SVM: Maximum margin separating hyperplane</em></a>,</li>
<li><a class="reference internal" href="../auto_examples/svm/plot_separating_hyperplane_unbalanced.html#example-svm-plot-separating-hyperplane-unbalanced-py"><em>SVM: Separating hyperplane for unbalanced classes</em></a></li>
<li><a class="reference internal" href="../auto_examples/svm/plot_svm_anova.html#example-svm-plot-svm-anova-py"><em>SVM-Anova: SVM with univariate feature selection</em></a>,</li>
<li><a class="reference internal" href="../auto_examples/svm/plot_svm_nonlinear.html#example-svm-plot-svm-nonlinear-py"><em>Non-linear SVM</em></a></li>
<li><a class="reference internal" href="../auto_examples/svm/plot_weighted_samples.html#example-svm-plot-weighted-samples-py"><em>SVM: Weighted samples</em></a>,</li>
</ul>
</div>
</div>
</div>
<div class="section" id="regression">
<span id="svm-regression"></span><h2>3.2.2. Regression<a class="headerlink" href="#regression" title="Permalink to this headline">¶</a></h2>
<p>The method of Support Vector Classification can be extended to solve
regression problems. This method is called Support Vector Regression.</p>
<p>The model produced by support vector classification (as described
above) depends only on a subset of the training data, because the cost
function for building the model does not care about training points
that lie beyond the margin. Analogously, the model produced by Support
Vector Regression depends only on a subset of the training data,
because the cost function for building the model ignores any training
data close to the model prediction.</p>
<p>There are two flavors of Support Vector Regression: <a class="reference internal" href="generated/sklearn.svm.SVR.html#sklearn.svm.SVR" title="sklearn.svm.SVR"><tt class="xref py py-class docutils literal"><span class="pre">SVR</span></tt></a> and
<a class="reference internal" href="generated/sklearn.svm.NuSVR.html#sklearn.svm.NuSVR" title="sklearn.svm.NuSVR"><tt class="xref py py-class docutils literal"><span class="pre">NuSVR</span></tt></a>.</p>
<p>As with classification classes, the fit method will take as
argument vectors X, y, only that in this case y is expected to have
floating point values instead of integer values:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">svm</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVR</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=0.5,</span>
<span class="go">  kernel=&#39;rbf&#39;, probability=False, scale_C=False, shrinking=True,</span>
<span class="go">  tol=0.001)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="go">array([ 1.5])</span>
</pre></div>
</div>
<div class="topic">
<p class="topic-title first">Examples:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/svm/plot_svm_regression.html#example-svm-plot-svm-regression-py"><em>Support Vector Regression (SVR) using linear and non-linear kernels</em></a></li>
</ul>
</div>
</div>
<div class="section" id="density-estimation-novelty-detection">
<span id="svm-outlier-detection"></span><h2>3.2.3. Density estimation, novelty detection<a class="headerlink" href="#density-estimation-novelty-detection" title="Permalink to this headline">¶</a></h2>
<p>One-class SVM is used for novelty detection, that is, given a set of
samples, it will detect the soft boundary of that set so as to
classify new points as belonging to that set or not. The class that
implements this is called <a class="reference internal" href="generated/sklearn.svm.OneClassSVM.html#sklearn.svm.OneClassSVM" title="sklearn.svm.OneClassSVM"><tt class="xref py py-class docutils literal"><span class="pre">OneClassSVM</span></tt></a>.</p>
<p>In this case, as it is a type of unsupervised learning, the fit method
will only take as input an array X, as there are no class labels.</p>
<p>See, section <a class="reference internal" href="outlier_detection.html#outlier-detection"><em>Novelty and Outlier Detection</em></a> for more details on this usage.</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/svm/plot_oneclass.html"><img alt="../_images/plot_oneclass_11.png" src="../_images/plot_oneclass_11.png" style="width: 600.0px; height: 450.0px;" /></a>
</div>
<div class="topic">
<p class="topic-title first">Examples:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/svm/plot_oneclass.html#example-svm-plot-oneclass-py"><em>One-class SVM with non-linear kernel (RBF)</em></a></li>
<li><a class="reference internal" href="../auto_examples/applications/plot_species_distribution_modeling.html#example-applications-plot-species-distribution-modeling-py"><em>Species distribution modeling</em></a></li>
</ul>
</div>
</div>
<div class="section" id="support-vector-machines-for-sparse-data">
<h2>3.2.4. Support Vector machines for sparse data<a class="headerlink" href="#support-vector-machines-for-sparse-data" title="Permalink to this headline">¶</a></h2>
<p>There is support for sparse data given in any matrix in a format
supported by scipy.sparse. Classes have the same name, just prefixed
by the <cite>sparse</cite> namespace, and take the same arguments, with the
exception of training and test data, which is expected to be in a
matrix format defined in scipy.sparse.</p>
<p>For maximum efficiency, use the CSR matrix format as defined in
<a class="reference external" href="http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html">scipy.sparse.csr_matrix</a>.</p>
<p>Implemented classes are <a class="reference internal" href="generated/sklearn.svm.sparse.SVC.html#sklearn.svm.sparse.SVC" title="sklearn.svm.sparse.SVC"><tt class="xref py py-class docutils literal"><span class="pre">SVC</span></tt></a>, <a class="reference internal" href="generated/sklearn.svm.sparse.NuSVC.html#sklearn.svm.sparse.NuSVC" title="sklearn.svm.sparse.NuSVC"><tt class="xref py py-class docutils literal"><span class="pre">NuSVC</span></tt></a>,
<a class="reference internal" href="generated/sklearn.svm.sparse.SVR.html#sklearn.svm.sparse.SVR" title="sklearn.svm.sparse.SVR"><tt class="xref py py-class docutils literal"><span class="pre">SVR</span></tt></a>, <a class="reference internal" href="generated/sklearn.svm.sparse.NuSVR.html#sklearn.svm.sparse.NuSVR" title="sklearn.svm.sparse.NuSVR"><tt class="xref py py-class docutils literal"><span class="pre">NuSVR</span></tt></a>, <a class="reference internal" href="generated/sklearn.svm.sparse.OneClassSVM.html#sklearn.svm.sparse.OneClassSVM" title="sklearn.svm.sparse.OneClassSVM"><tt class="xref py py-class docutils literal"><span class="pre">OneClassSVM</span></tt></a>,
<a class="reference internal" href="generated/sklearn.svm.sparse.LinearSVC.html#sklearn.svm.sparse.LinearSVC" title="sklearn.svm.sparse.LinearSVC"><tt class="xref py py-class docutils literal"><span class="pre">LinearSVC</span></tt></a>.</p>
</div>
<div class="section" id="complexity">
<h2>3.2.5. Complexity<a class="headerlink" href="#complexity" title="Permalink to this headline">¶</a></h2>
<p>Support Vector Machines are powerful tools, but their compute and
storage requirements increase rapidly with the number of training
vectors. The core of an SVM is a quadratic programming problem (QP),
separating support vectors from the rest of the training data. The QP
solver used by this <a class="reference external" href="http://www.csie.ntu.edu.tw/~cjlin/libsvm/">libsvm</a>-based implementation scales between
<img class="math" src="../_images/math/1f1311c79b186e9a1e4aabcf5892dffc661c0264.png" alt="O(n_{features} \times n_{samples}^2)"/> and
<img class="math" src="../_images/math/7fa7e2716acbd8ca70826fdacdbb446261ee7f51.png" alt="O(n_{features} \times n_{samples}^3)"/> depending on how efficiently
the <a class="reference external" href="http://www.csie.ntu.edu.tw/~cjlin/libsvm/">libsvm</a> cache is used in practice (dataset dependent). If the data
is very sparse <img class="math" src="../_images/math/02e494853b2b09e9c00e1c6b80864bc8b016bf5a.png" alt="n_{features}"/> should be replaced by the average number
of non-zero features in a sample vector.</p>
<p>Also note that for the linear case, the algorithm used in
<a class="reference internal" href="generated/sklearn.svm.sparse.LinearSVC.html#sklearn.svm.sparse.LinearSVC" title="sklearn.svm.sparse.LinearSVC"><tt class="xref py py-class docutils literal"><span class="pre">LinearSVC</span></tt></a> by the <a class="reference external" href="http://www.csie.ntu.edu.tw/~cjlin/liblinear/">liblinear</a> implementation is much more
efficient than its <a class="reference external" href="http://www.csie.ntu.edu.tw/~cjlin/libsvm/">libsvm</a>-based <a class="reference internal" href="generated/sklearn.svm.sparse.SVC.html#sklearn.svm.sparse.SVC" title="sklearn.svm.sparse.SVC"><tt class="xref py py-class docutils literal"><span class="pre">SVC</span></tt></a> counterpart and can
scale almost linearly to millions of samples and/or features.</p>
</div>
<div class="section" id="tips-on-practical-use">
<h2>3.2.6. Tips on Practical Use<a class="headerlink" href="#tips-on-practical-use" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul>
<li><p class="first"><strong>Avoiding data copy</strong>: For SVC, SVR, NuSVC and NuSVR, if the data
passed to certain methods is not C-ordered contiguous, and double
precision, it will be copied before calling the underlying C
implementation. You can check whether a give numpy array is
C-contiguous by inspecting its <cite>flags</cite> attribute.</p>
<p>For LinearSVC (and LogisticRegression) any input passed as a
numpy array will be copied and converted to the liblinear
internal sparse data representation (double precision floats
and int32 indices of non-zero components). If you want to fit
a large-scale linear classifier without copying a dense numpy
C-contiguous double precision array as input we suggest to use
the SGDClassifier class instead. The objective function can be
configured to be almost the same as the LinearSVC model.</p>
</li>
<li><p class="first"><strong>Kernel cache size</strong>: For SVC, SVR, nuSVC and NuSVR, the size of
the kernel cache has a strong impact on run times for larger
problems.  If you have enough RAM available, it is recommended to
set <cite>cache_size</cite> to a higher value than the default of 200(MB),
such as 500(MB) or 1000(MB).</p>
</li>
<li><p class="first">Support Vector Machine algorithms are not scale invariant, so <strong>it
is highly recommended to scale your data</strong>. For example, scale each
attribute on the input vector X to [0,1] or [-1,+1], or standardize it
to have mean 0 and variance 1. Note that the <em>same</em> scaling must be
applied to the test vector to obtain meaningful results. See section
<a class="reference internal" href="preprocessing.html#preprocessing"><em>Preprocessing data</em></a> for more details on scaling and normalization.</p>
</li>
<li><p class="first">Parameter nu in NuSVC/OneClassSVM/NuSVR approximates the fraction
of training errors and support vectors.</p>
</li>
<li><p class="first">In SVC, if data for classification are unbalanced (e.g. many
positive and few negative), set class_weight=&#8217;auto&#8217; and/or try
different penalty parameters C.</p>
</li>
<li><p class="first">The underlying <a class="reference internal" href="generated/sklearn.svm.sparse.LinearSVC.html#sklearn.svm.sparse.LinearSVC" title="sklearn.svm.sparse.LinearSVC"><tt class="xref py py-class docutils literal"><span class="pre">LinearSVC</span></tt></a> implementation uses a random
number generator to select features when fitting the model. It is
thus not uncommon, to have slightly different results for the same
input data. If that happens, try with a smaller tol parameter.</p>
</li>
<li><p class="first">Using L1 penalization as provided by LinearSVC(loss=&#8217;l2&#8217;,
penalty=&#8217;l1&#8217;, dual=False) yields a sparse solution, i.e. only a subset of
feature weights is different from zero and contribute to the decision
function.  Increasing C yields a more complex model (more feature are
selected).  The C value that yields a &#8220;null&#8221; model (all weights equal to
zero) can be calculated using <tt class="xref py py-func docutils literal"><span class="pre">l1_min_c</span></tt>.</p>
</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="kernel-functions">
<span id="svm-kernels"></span><h2>3.2.7. Kernel functions<a class="headerlink" href="#kernel-functions" title="Permalink to this headline">¶</a></h2>
<p>The <em>kernel function</em> can be any of the following:</p>
<blockquote>
<div><ul class="simple">
<li>linear: <img class="math" src="../_images/math/89236f7fdb8d5a6abe203db9593cab9b08b0b8fd.png" alt="&lt;x_i, x_j'&gt;"/>.</li>
<li>polynomial: <img class="math" src="../_images/math/8c848651b6dd9e1c081f50d8966208bcbacec38a.png" alt="(\gamma &lt;x, x'&gt; + r)^d"/>. d is specified by
keyword <cite>degree</cite>.</li>
<li>rbf (<img class="math" src="../_images/math/e5905fecf04a056f4580f94e9bc322ab2f593665.png" alt="exp(-\gamma |x-x'|^2), \gamma &gt; 0"/>). <img class="math" src="../_images/math/66981fa3920210c6ad8dbe5e968783d5dd7520c3.png" alt="\gamma"/> is
specified by keyword gamma.</li>
<li>sigmoid (<img class="math" src="../_images/math/a7a07268e39563daf2ade25aa1e0f279f6f6f458.png" alt="tanh(&lt;x_i,x_j&gt; + r)"/>).</li>
</ul>
</div></blockquote>
<p>Different kernels are specified by keyword kernel at initialization:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">linear_svc</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s">&#39;linear&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear_svc</span><span class="o">.</span><span class="n">kernel</span>
<span class="go">&#39;linear&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rbf_svc</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s">&#39;rbf&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rbf_svc</span><span class="o">.</span><span class="n">kernel</span>
<span class="go">&#39;rbf&#39;</span>
</pre></div>
</div>
<div class="section" id="custom-kernels">
<h3>3.2.7.1. Custom Kernels<a class="headerlink" href="#custom-kernels" title="Permalink to this headline">¶</a></h3>
<p>You can define your own kernels by either giving the kernel as a
python function or by precomputing the Gram matrix.</p>
<p>Classifiers with custom kernels behave the same way as any other
classifiers, except that:</p>
<blockquote>
<div><ul class="simple">
<li>Field <cite>support_vectors_</cite> is now empty, only indices of support
vectors are stored in <cite>support_</cite></li>
<li>A reference (and not a copy) of the first argument in the fit()
method is stored for future reference. If that array changes
between the use of fit() and predict() you will have unexpected
results.</li>
</ul>
</div></blockquote>
<div class="section" id="using-python-functions-as-kernels">
<h4>3.2.7.1.1. Using python functions as kernels<a class="headerlink" href="#using-python-functions-as-kernels" title="Permalink to this headline">¶</a></h4>
<p>You can also use your own defined kernels by passing a function to the
keyword <cite>kernel</cite> in the constructor.</p>
<p>Your kernel must take as arguments two matrices and return a third matrix.</p>
<p>The following code defines a linear kernel and creates a classifier
instance that will use that kernel:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">svm</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">my_kernel</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="n">my_kernel</span><span class="p">)</span>
</pre></div>
</div>
<div class="topic">
<p class="topic-title first">Examples:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/svm/plot_custom_kernel.html#example-svm-plot-custom-kernel-py"><em>SVM with custom kernel</em></a>.</li>
</ul>
</div>
</div>
<div class="section" id="using-the-gram-matrix">
<h4>3.2.7.1.2. Using the Gram matrix<a class="headerlink" href="#using-the-gram-matrix" title="Permalink to this headline">¶</a></h4>
<p>Set kernel=&#8217;precomputed&#8217; and pass the Gram matrix instead of X in the
fit method. At the moment, the kernel values between <cite>all</cite> training
vectors and the test vectors must be provided.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">svm</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s">&#39;precomputed&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c"># linear kernel computation</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gram</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">gram</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">SVC(C=1.0, cache_size=200, coef0=0.0, degree=3, gamma=0.0,</span>
<span class="go">  kernel=&#39;precomputed&#39;, probability=False, scale_C=False, shrinking=True,</span>
<span class="go">  tol=0.001)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c"># predict on training examples</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">gram</span><span class="p">)</span>
<span class="go">array([ 0.,  1.])</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="mathematical-formulation">
<span id="svm-mathematical-formulation"></span><h2>3.2.8. Mathematical formulation<a class="headerlink" href="#mathematical-formulation" title="Permalink to this headline">¶</a></h2>
<p>A support vector machine constructs a hyper-plane or set of hyper-planes
in a high or infinite dimensional space, which can be used for
classification, regression or other tasks. Intuitively, a good
separation is achieved by the hyper-plane that has the largest distance
to the nearest training data points of any class (so-called functional
margin), since in general the larger the margin the lower the
generalization error of the classifier.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/plot_separating_hyperplane_11.png"><img alt="../_images/plot_separating_hyperplane_11.png" src="../_images/plot_separating_hyperplane_11.png" style="width: 600.0px; height: 450.0px;" /></a>
</div>
<div class="section" id="svc">
<h3>3.2.8.1. SVC<a class="headerlink" href="#svc" title="Permalink to this headline">¶</a></h3>
<p>Given training vectors <img class="math" src="../_images/math/2ed46ef07a5512da9981a79295fbbf97bf769b0a.png" alt="x_i \in R^n"/>, i=1,..., l, in two
classes, and a vector <img class="math" src="../_images/math/3c58bccb2ba85f4ecd2a0339b151c28483aaea01.png" alt="y \in R^l"/> such that <img class="math" src="../_images/math/62d1091cef56573347ba66526e685759c228df94.png" alt="y_i \in {1,
-1}"/>, SVC solves the following primal problem:</p>
<div class="math">
<p><img src="../_images/math/eb74783f01c85187766959706f842a74224e10f4.png" alt="\min_ {w, b, \zeta} \frac{1}{2} w^T w + C \sum_{i=1, l} \zeta_i



\textrm {subject to } &amp; y_i (w^T \phi (x_i) + b) \geq 1 - \zeta_i,\\
&amp; \zeta_i \geq 0, i=1, ..., l"/></p>
</div><p>Its dual is</p>
<div class="math">
<p><img src="../_images/math/2e17355bcd769a4b9d93acb2eccb9c3c4151a3c1.png" alt="\min_{\alpha} \frac{1}{2} \alpha^T Q \alpha - e^T \alpha


\textrm {subject to } &amp; y^T \alpha = 0\\
&amp; 0 \leq \alpha_i \leq C, i=1, ..., l"/></p>
</div><p>where <img class="math" src="../_images/math/a3a59bb1293ee3f6dec19de4019a7178874219ae.png" alt="e"/> is the vector of all ones, C &gt; 0 is the upper bound, Q
is an l by l positive semidefinite matrix, <img class="math" src="../_images/math/20c63d702d950df0a5100366cf2227a3b0bb9e11.png" alt="Q_ij \equiv K(x_i,
x_j)"/> and <img class="math" src="../_images/math/2519dc33443f6b4bb0fd9cdc24ad7ba4e662ddea.png" alt="\phi (x_i)^T \phi (x)"/> is the kernel. Here training
vectors are mapped into a higher (maybe infinite) dimensional space by
the function <img class="math" src="../_images/math/2c175f60eecef1de7560c3bdea495d69f26f719d.png" alt="\phi"/></p>
<p>The decision function is:</p>
<div class="math">
<p><img src="../_images/math/ce974f73d12269df1c3ac89a4ae491686fe1e104.png" alt="sgn(\sum_{i=1}^l y_i \alpha_i K(x_i, x) + \rho)"/></p>
</div><p>This parameters can be accessed through the members <cite>dual_coef_</cite>
which holds the product <img class="math" src="../_images/math/91e0ab578768e5a3fdb5657751c33d9f167ef639.png" alt="y_i \alpha_i"/>, <cite>support_vectors_</cite> which
holds the support vectors, and <cite>intercept_</cite> which holds the independent
term <img class="math" src="../_images/math/55fa4f9455edd1c68ca06e7969c9c58d075ac409.png" alt="-\rho"/> :</p>
<div class="topic">
<p class="topic-title first">References:</p>
<ul class="simple">
<li><a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.17.7215">&#8220;Automatic Capacity Tuning of Very Large VC-dimension Classifiers&#8221;</a>
I Guyon, B Boser, V Vapnik - Advances in neural information
processing 1993,</li>
<li><a class="reference external" href="http://www.springerlink.com/content/k238jx04hm87j80g/">&#8220;Support-vector networks&#8221;</a>
C. Cortes, V. Vapnik, Machine Leaming, 20, 273-297 (1995)</li>
</ul>
</div>
</div>
<div class="section" id="nusvc">
<h3>3.2.8.2. NuSVC<a class="headerlink" href="#nusvc" title="Permalink to this headline">¶</a></h3>
<p>We introduce a new parameter <img class="math" src="../_images/math/d6a7ccf879c4a4fe694033606332cb83806db296.png" alt="\nu"/> which controls the number of
support vectors and training errors. The parameter <img class="math" src="../_images/math/948360fd417f20a48ae449aa26ede1baef981854.png" alt="\nu \in (0,
1]"/> is an upper bound on the fraction of training errors and a lower
bound of the fraction of support vectors.</p>
<p>It can be shown that the <cite>nu</cite>-SVC formulation is a reparametrization
of the <cite>C</cite>-SVC and therefore mathematically equivalent.</p>
</div>
</div>
<div class="section" id="implementation-details">
<h2>3.2.9. Implementation details<a class="headerlink" href="#implementation-details" title="Permalink to this headline">¶</a></h2>
<p>Internally, we use <a class="reference external" href="http://www.csie.ntu.edu.tw/~cjlin/libsvm/">libsvm</a> and <a class="reference external" href="http://www.csie.ntu.edu.tw/~cjlin/liblinear/">liblinear</a> to handle all
computations. These libraries are wrapped using C and Cython.</p>
<div class="topic">
<p class="topic-title first">References:</p>
<p>For a description of the implementation and details of the algorithms
used, please refer to</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference external" href="http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf">LIBSVM: a library for Support Vector Machines</a></li>
<li><a class="reference external" href="http://www.csie.ntu.edu.tw/~cjlin/liblinear/">LIBLINEAR &#8211; A Library for Large Linear Classification</a></li>
</ul>
</div></blockquote>
</div>
</div>
</div>


          </div>
        </div>
      </div>
        <div class="clearer"></div>
      </div>
    </div>

    <div class="footer">
        &copy; 2010–2011, scikit-learn developers (BSD License).
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.1.2. Design by <a href="http://webylimonada.com">Web y Limonada</a>.
    <span style="padding-left: 5ex;">
    <a href="../_sources/modules/svm.txt"
	    rel="nofollow">Show this page source</a>
    </span>
    </div>
  </body>
</html>