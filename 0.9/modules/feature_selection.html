

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>3.9. Feature selection &mdash; scikit-learn 0.9 documentation</title>
    
    <link rel="stylesheet" href="../_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '0.9',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="author" title="About these documents" href="../about.html" />
    <link rel="top" title="scikit-learn 0.9 documentation" href="../index.html" />
    <link rel="up" title="3. Supervised learning" href="../supervised_learning.html" />
    <link rel="next" title="4. Unsupervised learning" href="../unsupervised_learning.html" />
    <link rel="prev" title="3.8. Multiclass algorithms" href="multiclass.html" />
<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-22606712-2']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>

  </head>
  <body>
    <div class="header-wrapper">
      <div class="header">
          <p class="logo"><a href="../index.html">
            <img src="../_static/scikit-learn-logo-small.png" alt="Logo"/>
          </a>
          </p><div class="navbar">
          <ul>
            <li><a href="../install.html">Download</a></li>
            <li><a href="../support.html">Support</a></li>
            <li><a href="../user_guide.html">User Guide</a></li>
            <li><a href="../auto_examples/index.html">Examples</a></li>
            <li><a href="../developers/index.html">Development</a></li>
       </ul>

<div class="search_form">

<div id="cse" style="width: 100%;"></div>
<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">
  google.load('search', '1', {language : 'en'});
  google.setOnLoadCallback(function() {
    var customSearchControl = new google.search.CustomSearchControl('016639176250731907682:tjtqbvtvij0');
    customSearchControl.setResultSetSize(google.search.Search.FILTERED_CSE_RESULTSET);
    var options = new google.search.DrawOptions();
    options.setAutoComplete(true);
    customSearchControl.draw('cse', options);
  }, true);
</script>

</div>

          </div> <!-- end navbar --></div>
    </div>

    <div class="content-wrapper">

    <!-- <div id="blue_tile"></div> -->

        <div class="sphinxsidebar">
        <div class="rel">
          <a href="multiclass.html" title="3.8. Multiclass algorithms"
             accesskey="P">previous</a> |
          <a href="../unsupervised_learning.html" title="4. Unsupervised learning"
             accesskey="N">next</a> |
          <a href="../np-modindex.html" title="Python Module Index"
             >modules</a> |
          <a href="../py-modindex.html" title="Python Module Index"
             >modules</a> |
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a>
        </div>
        

        <h3>This page</h3>
         <ul>
<li><a class="reference internal" href="#">3.9. Feature selection</a><ul>
<li><a class="reference internal" href="#univariate-feature-selection">3.9.1. Univariate feature selection</a><ul>
<li><a class="reference internal" href="#feature-scoring-functions">3.9.1.1. Feature scoring functions</a><ul>
<li><a class="reference internal" href="#for-classification">3.9.1.1.1. For classification</a></li>
<li><a class="reference internal" href="#for-regression">3.9.1.1.2. For regression</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#recursive-feature-elimination">3.9.2. Recursive feature elimination</a></li>
<li><a class="reference internal" href="#l1-based-feature-selection">3.9.3. L1-based feature selection</a></li>
</ul>
</li>
</ul>


        

        <h3>Citing</h3>
        <p>Please consider
	<a href="about.html#citing-the-scikit-learn">citing the
	scikit-learn</a>.
        </div>

      <div class="content">
            
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="feature-selection">
<span id="id1"></span><h1>3.9. Feature selection<a class="headerlink" href="#feature-selection" title="Permalink to this headline">¶</a></h1>
<p>The classes in the <tt class="docutils literal"><span class="pre">sklearn.feature_selection</span></tt> module can be used
for feature selection/dimensionality reduction on sample sets, either to
improve estimators&#8217; accuracy scores or to boost their performance on very
high-dimensional datasets.</p>
<div class="section" id="univariate-feature-selection">
<h2>3.9.1. Univariate feature selection<a class="headerlink" href="#univariate-feature-selection" title="Permalink to this headline">¶</a></h2>
<p>Univariate feature selection works by selecting the best features based on
univariate statistical tests. It can seen as a preprocessing step
to an estimator. The <cite>scikit.learn</cite> exposes feature selection routines
a objects that implement the <cite>transform</cite> method. The k-best features
can be selected based on:</p>
<dl class="function">
<dt id="sklearn.feature_selection.univariate_selection.SelectKBest">
<tt class="descclassname">sklearn.feature_selection.univariate_selection.</tt><tt class="descname">SelectKBest</tt><big>(</big><em>score_func</em>, <em>k=10</em><big>)</big><a class="headerlink" href="#sklearn.feature_selection.univariate_selection.SelectKBest" title="Permalink to this definition">¶</a></dt>
<dd><p>Filter : Select the k lowest p-values</p>
</dd></dl>

<p>or by setting a percentile of features to keep using</p>
<dl class="function">
<dt id="sklearn.feature_selection.univariate_selection.SelectPercentile">
<tt class="descclassname">sklearn.feature_selection.univariate_selection.</tt><tt class="descname">SelectPercentile</tt><big>(</big><em>score_func</em>, <em>percentile=10</em><big>)</big><a class="headerlink" href="#sklearn.feature_selection.univariate_selection.SelectPercentile" title="Permalink to this definition">¶</a></dt>
<dd><p>Filter : Select the best percentile of the p_values</p>
</dd></dl>

<p>or using common univariate statistical test for each feature:</p>
<dl class="function">
<dt id="sklearn.feature_selection.univariate_selection.SelectFpr">
<tt class="descclassname">sklearn.feature_selection.univariate_selection.</tt><tt class="descname">SelectFpr</tt><big>(</big><em>score_func</em>, <em>alpha=0.050000000000000003</em><big>)</big><a class="headerlink" href="#sklearn.feature_selection.univariate_selection.SelectFpr" title="Permalink to this definition">¶</a></dt>
<dd><p>Filter : Select the pvalues below alpha based on a FPR test: False
Positive Rate: controlling the total amount of false detections.</p>
</dd></dl>

<dl class="function">
<dt id="sklearn.feature_selection.univariate_selection.SelectFdr">
<tt class="descclassname">sklearn.feature_selection.univariate_selection.</tt><tt class="descname">SelectFdr</tt><big>(</big><em>score_func</em>, <em>alpha=0.050000000000000003</em><big>)</big><a class="headerlink" href="#sklearn.feature_selection.univariate_selection.SelectFdr" title="Permalink to this definition">¶</a></dt>
<dd><p>Filter : Select the p-values corresponding to an estimated false
discovery rate of alpha. This uses the Benjamini-Hochberg procedure</p>
</dd></dl>

<dl class="function">
<dt id="sklearn.feature_selection.univariate_selection.SelectFwe">
<tt class="descclassname">sklearn.feature_selection.univariate_selection.</tt><tt class="descname">SelectFwe</tt><big>(</big><em>score_func</em>, <em>alpha=0.050000000000000003</em><big>)</big><a class="headerlink" href="#sklearn.feature_selection.univariate_selection.SelectFwe" title="Permalink to this definition">¶</a></dt>
<dd><p>Filter : Select the p-values corresponding to Family-wise error rate: a
corrected p-value of alpha</p>
</dd></dl>

<p>These objects take as input a scoring function that returns
univariate p-values.</p>
<div class="topic">
<p class="topic-title first">Examples:</p>
<p><a class="reference internal" href="../auto_examples/plot_feature_selection.html#example-plot-feature-selection-py"><em>Univariate Feature Selection</em></a></p>
</div>
<div class="section" id="feature-scoring-functions">
<h3>3.9.1.1. Feature scoring functions<a class="headerlink" href="#feature-scoring-functions" title="Permalink to this headline">¶</a></h3>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Beware not to use a regression scoring function with a classification problem.</p>
</div>
<div class="section" id="for-classification">
<h4>3.9.1.1.1. For classification<a class="headerlink" href="#for-classification" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="sklearn.feature_selection.univariate_selection.chi2">
<tt class="descclassname">sklearn.feature_selection.univariate_selection.</tt><tt class="descname">chi2</tt><big>(</big><em>X</em>, <em>y</em><big>)</big><a class="headerlink" href="#sklearn.feature_selection.univariate_selection.chi2" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute χ² (chi-squared) statistic for each class/feature combination.</p>
<p>This transformer can be used to select the n_features features with the
highest values for the χ² (chi-square) statistic from either boolean or
multinomially distributed data (e.g., term counts in document
classification) relative to the classes.</p>
<p>Recall that the χ² statistic measures dependence between stochastic
variables, so a transformer based on this function &#8220;weeds out&#8221; the features
that are the most likely to be independent of class and therefore
irrelevant for classification.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters :</th><td class="field-body"><p class="first"><strong>X</strong> : {array-like, sparse matrix}, shape = [n_samples, n_features_in]</p>
<blockquote>
<div><p>Sample vectors.</p>
</div></blockquote>
<p><strong>y</strong> : array-like, shape = n_samples</p>
<blockquote class="last">
<div><p>Target vector (class labels).</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="sklearn.feature_selection.univariate_selection.f_classif">
<tt class="descclassname">sklearn.feature_selection.univariate_selection.</tt><tt class="descname">f_classif</tt><big>(</big><em>X</em>, <em>y</em><big>)</big><a class="headerlink" href="#sklearn.feature_selection.univariate_selection.f_classif" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the Anova F-value for the provided sample</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters :</th><td class="field-body"><p class="first"><strong>X</strong> : array of shape (n_samples, n_features)</p>
<blockquote>
<div><p>the set of regressors sthat will tested sequentially</p>
</div></blockquote>
<p><strong>y</strong> : array of shape(n_samples)</p>
<blockquote>
<div><p>the data matrix</p>
</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns :</th><td class="field-body"><p class="first"><strong>F</strong> : array of shape (m),</p>
<blockquote>
<div><p>the set of F values</p>
</div></blockquote>
<p><strong>pval</strong> : array of shape(m),</p>
<blockquote class="last">
<div><p>the set of p-values</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<div class="topic">
<p class="topic-title first">Feature selection with sparse data</p>
<p>If you use sparse data (i.e. data represented as sparse matrices),
only <a class="reference internal" href="generated/sklearn.feature_selection.univariate_selection.chi2.html#sklearn.feature_selection.univariate_selection.chi2" title="sklearn.feature_selection.univariate_selection.chi2"><tt class="xref py py-func docutils literal"><span class="pre">chi2</span></tt></a> will deal with the data without making it dense.</p>
</div>
</div>
<div class="section" id="for-regression">
<h4>3.9.1.1.2. For regression<a class="headerlink" href="#for-regression" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="sklearn.feature_selection.univariate_selection.f_regression">
<tt class="descclassname">sklearn.feature_selection.univariate_selection.</tt><tt class="descname">f_regression</tt><big>(</big><em>X</em>, <em>y</em>, <em>center=True</em><big>)</big><a class="headerlink" href="#sklearn.feature_selection.univariate_selection.f_regression" title="Permalink to this definition">¶</a></dt>
<dd><p>Quick linear model for testing the effect of a single regressor,
sequentially for many regressors
This is done in 3 steps:
1. the regressor of interest and the data are orthogonalized
wrt constant regressors
2. the cross correlation between data and regressors is computed
3. it is converted to an F score then to a p-value</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters :</th><td class="field-body"><p class="first"><strong>X</strong> : array of shape (n_samples, n_features)</p>
<blockquote>
<div><p>the set of regressors sthat will tested sequentially</p>
</div></blockquote>
<p><strong>y</strong> : array of shape(n_samples)</p>
<blockquote>
<div><p>the data matrix</p>
</div></blockquote>
<p><strong>center</strong> : True, bool,</p>
<blockquote>
<div><p>If true, X and y are centered</p>
</div></blockquote>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns :</th><td class="field-body"><p class="first"><strong>F</strong> : array of shape (m),</p>
<blockquote>
<div><p>the set of F values</p>
</div></blockquote>
<p><strong>pval</strong> : array of shape(m)</p>
<blockquote class="last">
<div><p>the set of p-values</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
</div>
</div>
<div class="section" id="recursive-feature-elimination">
<h2>3.9.2. Recursive feature elimination<a class="headerlink" href="#recursive-feature-elimination" title="Permalink to this headline">¶</a></h2>
<p>Given an external estimator that assigns weights to features (e.g., the
coefficients of a linear model), the goal of recursive feature elimination (RFE)
is to select features by recursively considering smaller and smaller sets of
features.  First, the estimator is trained on the initial set of features and
weights are assigned to each one of them. Then, features whose absolute weights
are the smallest are pruned from the current set features. That procedure is
recursively repeated on the pruned set until the desired number of features to
select is eventually reached.</p>
<div class="topic">
<p class="topic-title first">Examples:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/plot_rfe_digits.html#example-plot-rfe-digits-py"><em>Recursive feature elimination</em></a>: A recursive feature elimination example
showing the relevance of pixels in a digit classification task.</li>
<li><a class="reference internal" href="../auto_examples/plot_rfe_with_cross_validation.html#example-plot-rfe-with-cross-validation-py"><em>Recursive feature elimination with cross-validation</em></a>: A recursive feature
elimination example with automatic tuning of the number of features
selected with cross-validation.</li>
</ul>
</div>
</div>
<div class="section" id="l1-based-feature-selection">
<h2>3.9.3. L1-based feature selection<a class="headerlink" href="#l1-based-feature-selection" title="Permalink to this headline">¶</a></h2>
<p>Linear models penalized with the L1 norm have sparse solutions. When the goal
is to reduce the dimensionality of the data to use with another classifier, the
<cite>transform</cite> method of <cite>LogisticRegression</cite> and <cite>LinearSVC</cite> can be used:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">LinearSVC</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(150, 4)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_new</span> <span class="o">=</span> <span class="n">LinearSVC</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s">&quot;l1&quot;</span><span class="p">,</span> <span class="n">dual</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_new</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(150, 2)</span>
</pre></div>
</div>
<p>The parameter C controls the sparsity: the smaller the fewer features.</p>
<div class="topic">
<p class="topic-title first">Examples:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/document_classification_20newsgroups.html#example-document-classification-20newsgroups-py"><em>Classification of text documents using sparse features</em></a>: Comparison
of different algorithms for document classification including L1-based
feature selection.</li>
</ul>
</div>
</div>
</div>


          </div>
        </div>
      </div>
        <div class="clearer"></div>
      </div>
    </div>

    <div class="footer">
        <p style="text-align: center">This documentation is relative
        to scikit-learn version 0.9<p>
        &copy; 2010–2011, scikit-learn developers (BSD License).
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.1.2. Design by <a href="http://webylimonada.com">Web y Limonada</a>.
    <span style="padding-left: 5ex;">
    <a href="../_sources/modules/feature_selection.txt"
	    rel="nofollow">Show this page source</a>
    </span>
    </div>
  </body>
</html>