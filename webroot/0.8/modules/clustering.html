

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>4.2. Clustering &mdash; scikits.learn v0.8 documentation</title>
    <link rel="stylesheet" href="../_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '0.8',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="author" title="About these documents" href="../about.html" />
    <link rel="top" title="scikits.learn v0.8 documentation" href="../index.html" />
    <link rel="up" title="4. Unsupervised learning" href="../unsupervised_learning.html" />
    <link rel="next" title="4.3. Decomposing signals in components (matrix factorization problems)" href="decomposition.html" />
    <link rel="prev" title="4.1. Gaussian mixture models" href="mixture.html" />
<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-22606712-2']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>

  </head>
  <body>
    <div class="header-wrapper">
      <div class="header">
          <p class="logo"><a href="../index.html">
            <img src="../_static/scikit-learn-logo-small.png" alt="Logo"/>
          </a>
          </p><div class="navbar">
          <ul>
            <li><a href="../install.html">Download</a></li>
            <li><a href="../support.html">Support</a></li>
            <li><a href="../user_guide.html">User Guide</a></li>
            <li><a href="../auto_examples/index.html">Examples</a></li>
            <li><a href="../developers/index.html">Development</a></li>
       </ul>

<div class="search_form">

<div id="cse" style="width: 100%;"></div>
<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">
  google.load('search', '1', {language : 'en'});
  google.setOnLoadCallback(function() {
    var customSearchControl = new google.search.CustomSearchControl('016639176250731907682:tjtqbvtvij0');
    customSearchControl.setResultSetSize(google.search.Search.FILTERED_CSE_RESULTSET);
    var options = new google.search.DrawOptions();
    options.setAutoComplete(true);
    customSearchControl.draw('cse', options);
  }, true);
</script>

</div>

          </div> <!-- end navbar --></div>
    </div>

    <div class="content-wrapper">

    <!-- <div id="blue_tile"></div> -->

        <div class="sphinxsidebar">
        <div class="rel">
          <a href="mixture.html" title="4.1. Gaussian mixture models"
             accesskey="P">previous</a> |
          <a href="decomposition.html" title="4.3. Decomposing signals in components (matrix factorization problems)"
             accesskey="N">next</a> |
          <a href="../np-modindex.html" title="Python Module Index"
             >modules</a> |
          <a href="../py-modindex.html" title="Python Module Index"
             >modules</a> |
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a>
        </div>
        

        <h3>This page</h3>
         <ul>
<li><a class="reference internal" href="#">4.2. Clustering</a><ul>
<li><a class="reference internal" href="#k-means">4.2.1. K-means</a></li>
<li><a class="reference internal" href="#affinity-propagation">4.2.2. Affinity propagation</a></li>
<li><a class="reference internal" href="#mean-shift">4.2.3. Mean Shift</a></li>
<li><a class="reference internal" href="#spectral-clustering">4.2.4. Spectral clustering</a></li>
<li><a class="reference internal" href="#hierarchical-clustering">4.2.5. Hierarchical clustering</a><ul>
<li><a class="reference internal" href="#adding-connectivity-constraints">4.2.5.1. Adding connectivity constraints</a></li>
</ul>
</li>
</ul>
</li>
</ul>


        

        </div>

      <div class="content">
            
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="clustering">
<span id="id1"></span><h1>4.2. Clustering<a class="headerlink" href="#clustering" title="Permalink to this headline">¶</a></h1>
<p><a class="reference external" href="http://en.wikipedia.org/wiki/Cluster_analysis">Clustering</a> of
unlabeled data can be performed with the module <a class="reference internal" href="classes.html#module-scikits.learn.cluster"><tt class="xref py py-mod docutils literal"><span class="pre">scikits.learn.cluster</span></tt></a>.</p>
<p>Each clustering algorithm comes in two variants: a class, that implements
the <cite>fit</cite> method to learn the clusters on train data, and a function,
that, given train data, returns an array of integer labels corresponding
to the different clusters. For the class, the labels over the training
data can be found in the <cite>labels_</cite> attribute.</p>
<div class="topic">
<p class="topic-title first">Input data</p>
<p>One important thing to note is that the algorithms implemented in
this module take different kinds of matrix as input.  On one hand,
<a class="reference internal" href="generated/scikits.learn.cluster.MeanShift.html#scikits.learn.cluster.MeanShift" title="scikits.learn.cluster.MeanShift"><tt class="xref py py-class docutils literal"><span class="pre">MeanShift</span></tt></a> and <a class="reference internal" href="generated/scikits.learn.cluster.KMeans.html#scikits.learn.cluster.KMeans" title="scikits.learn.cluster.KMeans"><tt class="xref py py-class docutils literal"><span class="pre">KMeans</span></tt></a> take data matrices of shape
[n_samples, n_features]. These can be obtained from the classes in
the <a class="reference internal" href="classes.html#module-scikits.learn.feature_extraction"><tt class="xref py py-mod docutils literal"><span class="pre">scikits.learn.feature_extraction</span></tt></a> module. On the other hand,
<a class="reference internal" href="generated/scikits.learn.cluster.AffinityPropagation.html#scikits.learn.cluster.AffinityPropagation" title="scikits.learn.cluster.AffinityPropagation"><tt class="xref py py-class docutils literal"><span class="pre">AffinityPropagation</span></tt></a> and <a class="reference internal" href="generated/scikits.learn.cluster.SpectralClustering.html#scikits.learn.cluster.SpectralClustering" title="scikits.learn.cluster.SpectralClustering"><tt class="xref py py-class docutils literal"><span class="pre">SpectralClustering</span></tt></a> take
similarity matrices of shape [n_samples, n_samples].  These can be
obtained from the functions in the <a class="reference internal" href="classes.html#module-scikits.learn.metrics.pairwise"><tt class="xref py py-mod docutils literal"><span class="pre">scikits.learn.metrics.pairwise</span></tt></a>
module. In other words, <a class="reference internal" href="generated/scikits.learn.cluster.MeanShift.html#scikits.learn.cluster.MeanShift" title="scikits.learn.cluster.MeanShift"><tt class="xref py py-class docutils literal"><span class="pre">MeanShift</span></tt></a> and <a class="reference internal" href="generated/scikits.learn.cluster.KMeans.html#scikits.learn.cluster.KMeans" title="scikits.learn.cluster.KMeans"><tt class="xref py py-class docutils literal"><span class="pre">KMeans</span></tt></a> work
with points in a vector space, whereas <a class="reference internal" href="generated/scikits.learn.cluster.AffinityPropagation.html#scikits.learn.cluster.AffinityPropagation" title="scikits.learn.cluster.AffinityPropagation"><tt class="xref py py-class docutils literal"><span class="pre">AffinityPropagation</span></tt></a>
and <a class="reference internal" href="generated/scikits.learn.cluster.SpectralClustering.html#scikits.learn.cluster.SpectralClustering" title="scikits.learn.cluster.SpectralClustering"><tt class="xref py py-class docutils literal"><span class="pre">SpectralClustering</span></tt></a> can work with arbitrary objects, as
long as a similarity measure exists for such objects.</p>
</div>
<div class="section" id="k-means">
<span id="id2"></span><h2>4.2.1. K-means<a class="headerlink" href="#k-means" title="Permalink to this headline">¶</a></h2>
<p>The <a class="reference internal" href="generated/scikits.learn.cluster.KMeans.html#scikits.learn.cluster.KMeans" title="scikits.learn.cluster.KMeans"><tt class="xref py py-class docutils literal"><span class="pre">KMeans</span></tt></a> algorithm clusters data by trying to separate samples
in n groups of equal variance, minimizing a criterion known as the
&#8216;inertia&#8217; of the groups. This algorithm requires the number of cluster to
be specified. It scales well to large number of samples, however its
results may be dependent on an initialisation.</p>
</div>
<div class="section" id="affinity-propagation">
<h2>4.2.2. Affinity propagation<a class="headerlink" href="#affinity-propagation" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="generated/scikits.learn.cluster.AffinityPropagation.html#scikits.learn.cluster.AffinityPropagation" title="scikits.learn.cluster.AffinityPropagation"><tt class="xref py py-class docutils literal"><span class="pre">AffinityPropagation</span></tt></a> clusters data by diffusion in the similarity
matrix. This algorithm automatically sets its numbers of cluster. It
will have difficulties scaling to thousands of samples.</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/cluster/plot_affinity_propagation.html"><img alt="../_images/plot_affinity_propagation_1.png" src="../_images/plot_affinity_propagation_1.png" style="width: 400.0px; height: 300.0px;" /></a>
</div>
<div class="topic">
<p class="topic-title first">Examples:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/cluster/plot_affinity_propagation.html#example-cluster-plot-affinity-propagation-py"><em>Demo of affinity propagation clustering algorithm</em></a>: Affinity
Propagation on a synthetic 2D datasets with 3 classes.</li>
<li><a class="reference internal" href="../auto_examples/applications/stock_market.html#example-applications-stock-market-py"><em>Finding structure in the stock market</em></a> Affinity Propagation on Financial
time series to find groups of companies</li>
</ul>
</div>
</div>
<div class="section" id="mean-shift">
<h2>4.2.3. Mean Shift<a class="headerlink" href="#mean-shift" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="generated/scikits.learn.cluster.MeanShift.html#scikits.learn.cluster.MeanShift" title="scikits.learn.cluster.MeanShift"><tt class="xref py py-class docutils literal"><span class="pre">MeanShift</span></tt></a> clusters data by estimating <em>blobs</em> in a smooth
density of points matrix. This algorithm automatically sets its numbers
of cluster. It will have difficulties scaling to thousands of samples.</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/cluster/plot_mean_shift.html"><img alt="../_images/plot_mean_shift_1.png" src="../_images/plot_mean_shift_1.png" style="width: 400.0px; height: 300.0px;" /></a>
</div>
<div class="topic">
<p class="topic-title first">Examples:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/cluster/plot_mean_shift.html#example-cluster-plot-mean-shift-py"><em>A demo of the mean-shift clustering algorithm</em></a>: Mean Shift clustering
on a synthetic 2D datasets with 3 classes.</li>
</ul>
</div>
</div>
<div class="section" id="spectral-clustering">
<h2>4.2.4. Spectral clustering<a class="headerlink" href="#spectral-clustering" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="generated/scikits.learn.cluster.SpectralClustering.html#scikits.learn.cluster.SpectralClustering" title="scikits.learn.cluster.SpectralClustering"><tt class="xref py py-class docutils literal"><span class="pre">SpectralClustering</span></tt></a> does a low-dimension embedding of the
affinity matrix between samples, followed by a KMeans in the low
dimensional space. It is especially efficient if the affinity matrix is
sparse and the <a class="reference external" href="http://code.google.com/p/pyamg/">pyamg</a> module is
installed. SpectralClustering requires the number of clusters to be
specified. It works well for a small number of clusters but is not
advised when using many clusters.</p>
<p>For two clusters, it solves a convex relaxation of the <a class="reference external" href="http://www.cs.berkeley.edu/~malik/papers/SM-ncut.pdf">normalised
cuts</a> problem on
the similarity graph: cutting the graph in two so that the weight of the
edges cut is small compared to the weights in of edges inside each
cluster. This criteria is especially interesting when working on images:
graph vertices are pixels, and edges of the similarity graph are a
function of the gradient of the image.</p>
<p class="centered">
<strong><a class="reference external image-reference" href="../auto_examples/cluster/plot_segmentation_toy.html"><img alt="noisy_img" src="../_images/plot_segmentation_toy_1.png" style="width: 300.0px; height: 300.0px;" /></a>
 <a class="reference external image-reference" href="../auto_examples/cluster/plot_segmentation_toy.html"><img alt="segmented_img" src="../_images/plot_segmentation_toy_2.png" style="width: 300.0px; height: 300.0px;" /></a>
</strong></p><div class="topic">
<p class="topic-title first">Examples:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/cluster/plot_segmentation_toy.html#example-cluster-plot-segmentation-toy-py"><em>Spectral clustering for image segmentation</em></a>: Segmenting objects
from a noisy background using spectral clustering.</li>
<li><a class="reference internal" href="../auto_examples/cluster/plot_lena_segmentation.html#example-cluster-plot-lena-segmentation-py"><em>Segmenting the picture of Lena in regions</em></a>: Spectral clustering
to split the image of lena in regions.</li>
</ul>
</div>
<div class="topic">
<p class="topic-title first">References:</p>
<ul class="simple">
<li><a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.165.9323">&#8220;A Tutorial on Spectral Clustering&#8221;</a>
Ulrike von Luxburg, 2007</li>
<li><a class="reference external" href="http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.160.2324">&#8220;Normalized cuts and image segmentation&#8221;</a>
Jianbo Shi, Jitendra Malik, 2000</li>
<li><a class="reference external" href="http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.33.1501">&#8220;A Random Walks View of Spectral Segmentation&#8221;</a>
Marina Meila, Jianbo Shi, 2001</li>
<li><a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.19.8100">&#8220;On Spectral Clustering: Analysis and an algorithm&#8221;</a>
Andrew Y. Ng, Michael I. Jordan, Yair Weiss, 2001</li>
</ul>
</div>
</div>
<div class="section" id="hierarchical-clustering">
<span id="id3"></span><h2>4.2.5. Hierarchical clustering<a class="headerlink" href="#hierarchical-clustering" title="Permalink to this headline">¶</a></h2>
<p>Hierarchical clustering is a general family of clustering algorithms that
build nested clusters by merging them successively. This hierarchy of
clusters represented as a tree (or dendrogram). The root of the tree is
the unique cluster that gathers all the samples, the leaves being the
clusters with only one sample. See the <a class="reference external" href="http://en.wikipedia.org/wiki/Hierarchical_clustering">Wikipedia page</a> for more
details.</p>
<p>The <a class="reference internal" href="generated/scikits.learn.cluster.Ward.html#scikits.learn.cluster.Ward" title="scikits.learn.cluster.Ward"><tt class="xref py py-class docutils literal"><span class="pre">Ward</span></tt></a> object performs a hierarchical clustering based on
the Ward algorithm, that is a variance-minimizing approach. At each
step, it minimizes the sum of squared differences within all clusters
(inertia criterion).</p>
<p>This algorithm can scale to large number of samples when it is used jointly
with an connectivity matrix, but can be computationally expensive when no
connectivity constraints are added between samples: it considers at each step
all the possible merges.</p>
<div class="section" id="adding-connectivity-constraints">
<h3>4.2.5.1. Adding connectivity constraints<a class="headerlink" href="#adding-connectivity-constraints" title="Permalink to this headline">¶</a></h3>
<p>An interesting aspect of the <a class="reference internal" href="generated/scikits.learn.cluster.Ward.html#scikits.learn.cluster.Ward" title="scikits.learn.cluster.Ward"><tt class="xref py py-class docutils literal"><span class="pre">Ward</span></tt></a> object is that connectivity
constraints can be added to this algorithm (only adjacent clusters can be
merged together), through an connectivity matrix that defines for each
sample the neighboring samples following a given structure of the data. For
instance, in the swiss-roll example below, the connectivity constraints
forbid the merging of points that are not adjacent on the swiss roll, and
thus avoid forming clusters that extend across overlapping folds of the
roll.</p>
<p class="centered">
<strong><a class="reference external image-reference" href="../auto_examples/cluster/plot_ward_structured_vs_unstructured.html"><img alt="unstructured" src="../_images/plot_ward_structured_vs_unstructured_1.png" style="width: 400.0px; height: 300.0px;" /></a>
 <a class="reference external image-reference" href="../auto_examples/cluster/plot_ward_structured_vs_unstructured.html"><img alt="structured" src="../_images/plot_ward_structured_vs_unstructured_2.png" style="width: 400.0px; height: 300.0px;" /></a>
</strong></p><p>The connectivity constraints are imposed via an connectivity matrix: a
scipy sparse matrix that has elements only at the intersection of a row
and a column with indices of the dataset that should be connected. This
matrix can be constructed from apriori information, for instance if you
whish to cluster web pages, but only merging pages with a link pointing
from one to another. It can also be learned from the data, for instance
using <a class="reference internal" href="generated/scikits.learn.neighbors.kneighbors_graph.html#scikits.learn.neighbors.kneighbors_graph" title="scikits.learn.neighbors.kneighbors_graph"><tt class="xref py py-func docutils literal"><span class="pre">scikits.learn.neighbors.kneighbors_graph</span></tt></a> to restrict
merging to nearest neighbors as in the <a class="reference internal" href="../auto_examples/cluster/plot_ward_structured_vs_unstructured.html#example-cluster-plot-ward-structured-vs-unstructured-py"><em>swiss roll</em></a> example, or
using <a class="reference internal" href="generated/scikits.learn.feature_extraction.image.grid_to_graph.html#scikits.learn.feature_extraction.image.grid_to_graph" title="scikits.learn.feature_extraction.image.grid_to_graph"><tt class="xref py py-func docutils literal"><span class="pre">scikits.learn.feature_extraction.image.grid_to_graph</span></tt></a> to
enable only merging of neighboring pixels on an image, as in the
<a class="reference internal" href="../auto_examples/cluster/plot_lena_ward_segmentation.html#example-cluster-plot-lena-ward-segmentation-py"><em>Lena</em></a> example.</p>
<div class="topic">
<p class="topic-title first">Examples:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/cluster/plot_lena_ward_segmentation.html#example-cluster-plot-lena-ward-segmentation-py"><em>A demo of structured Ward hierarchical clustering on Lena image</em></a>: Ward clustering
to split the image of lena in regions.</li>
<li><a class="reference internal" href="../auto_examples/cluster/plot_ward_structured_vs_unstructured.html#example-cluster-plot-ward-structured-vs-unstructured-py"><em>Hierarchical clustering: structured vs unstructured ward</em></a>: Example of
Ward algorithm on a swiss-roll, comparison of structured approaches
versus unstructured approaches.</li>
<li><a class="reference internal" href="../auto_examples/cluster/plot_feature_agglomeration_vs_univariate_selection.html#example-cluster-plot-feature-agglomeration-vs-univariate-selection-py"><em>Feature agglomeration vs. univariate selection</em></a>:
Example of dimensionality reduction with feature agglomeration based on
Ward hierarchical clustering.</li>
</ul>
</div>
</div>
</div>
</div>


          </div>
        </div>
      </div>
        <div class="clearer"></div>
      </div>
    </div>

    <div class="footer">
        <p style="text-align: center">This documentation is relative
        to scikits.learn version 0.8<p>
        &copy; 2010, scikits.learn developers (BSD License).
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.0. Design by <a href="http://webylimonada.com">Web y Limonada</a>.
    <span style="padding-left: 5ex;">
    <a href="../_sources/modules/clustering.txt"
	    rel="nofollow">Show this page source</a>
    </span>
    </div>
  </body>
</html>