

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Explicit feature map approximation for RBF kernels &mdash; scikit-learn 0.11-git documentation</title>
    
    <link rel="stylesheet" href="../_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '0.11-git',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="author" title="About these documents" href="../about.html" />
    <link rel="top" title="scikit-learn 0.11-git documentation" href="../index.html" />
    <link rel="up" title="Examples" href="index.html" />
    <link rel="next" title="Linear and Quadratic Discriminant Analysis with confidence ellipsoid" href="plot_lda_qda.html" />
    <link rel="prev" title="Univariate Feature Selection" href="plot_feature_selection.html" />
<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-22606712-2']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>

  </head>
  <body>
    <div class="header-wrapper">
      <div class="header">
          <p class="logo"><a href="../index.html">
            <img src="../_static/scikit-learn-logo-small.png" alt="Logo"/>
          </a>
          </p><div class="navbar">
          <ul>
            <li><a href="../install.html">Download</a></li>
            <li><a href="../support.html">Support</a></li>
            <li><a href="../user_guide.html">User Guide</a></li>
            <li><a href="index.html">Examples</a></li>
            <li><a href="../modules/classes.html">Reference</a></li>
       </ul>

<div class="search_form">

<div id="cse" style="width: 100%;"></div>
<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">
  google.load('search', '1', {language : 'en'});
  google.setOnLoadCallback(function() {
    var customSearchControl = new google.search.CustomSearchControl('016639176250731907682:tjtqbvtvij0');
    customSearchControl.setResultSetSize(google.search.Search.FILTERED_CSE_RESULTSET);
    var options = new google.search.DrawOptions();
    options.setAutoComplete(true);
    customSearchControl.draw('cse', options);
  }, true);
</script>

</div>
          </div> <!-- end navbar --></div>
    </div>

    <div class="content-wrapper">

    <div class="sphinxsidebar">
	<div class="rel">
	
	<!-- rellinks[1:] is an ugly hack to avoid link to module
	    index  -->
	<div class="rellink">
	<a href="plot_feature_selection.html" title="Univariate Feature Selection"
	    accesskey="P">Previous
	    <br>
	    <span class="smallrellink">
	    Univariate Featu...
	    </span>
	    <span class="hiddenrellink">
	    Univariate Feature Selection
	    </span>
	    
	    </a>
	</div>
	    <div class="spacer">
	    &nbsp;
	    </div>
	
	<div class="rellink">
	<a href="plot_lda_qda.html" title="Linear and Quadratic Discriminant Analysis with confidence ellipsoid"
	    accesskey="N">Next
	    <br>
	    <span class="smallrellink">
	    Linear and Quadr...
	    </span>
	    <span class="hiddenrellink">
	    Linear and Quadratic Discriminant Analysis with confidence ellipsoid
	    </span>
	    
	    </a>
	</div>
	<!-- Ad a link to the 'up' page -->
	<div class="spacer">
	&nbsp;
	</div>
	<div class="rellink">
	<a href="index.html" title="Examples" >
	Up
	<br>
	<span class="smallrellink">
	Examples
	</span>
	<span class="hiddenrellink">
	Examples
	</span>
	
	</a>
	</div>
    </div>
    <p style="text-align: center">This documentation is
    for scikit-learn <strong>version 0.11-git</strong>
    &mdash; <a href="http://scikit-learn.org/stable/support.html#documentation-resources">Other versions</a></p>
    
    <h3>Citing</h3>
    <p>If you use the software, please consider
    <a href="about.html#citing-scikit-learn">citing scikit-learn</a>.</p>
    <h3>This page</h3>
	<ul>
<li><a class="reference internal" href="#">Explicit feature map approximation for RBF kernels</a></li>
</ul>

    
    </div>

      <div class="content">
            
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="explicit-feature-map-approximation-for-rbf-kernels">
<span id="example-plot-kernel-approximation-py"></span><h1>Explicit feature map approximation for RBF kernels<a class="headerlink" href="#explicit-feature-map-approximation-for-rbf-kernels" title="Permalink to this headline">Â¶</a></h1>
<p>An example shows how to use <a class="reference internal" href="../modules/generated/sklearn.kernel_approximation.RBFSampler.html#sklearn.kernel_approximation.RBFSampler" title="sklearn.kernel_approximation.RBFSampler"><tt class="xref py py-class docutils literal"><span class="pre">RBFSampler</span></tt></a> to appoximate the feature map
of an RBF kernel for classification with an SVM on the digits dataset.  Results
using a linear SVM in the original space, a linear SVM using the approximate
mapping and using a kernelized SVM are compared.  Timings and accuracy for
varying amounts of Monte Carlo samplings for the approximate mapping are shown.</p>
<p>Sampling more dimensions clearly leads to better classification results, but
comes at a greater cost. This means there is a tradeoff between runtime and
accuracy, given by the parameter n_components.  Note that solving the Linear
SVM and also the approximate kernel SVM could be greatly accelerated by using
stochastic gradient descent via <a class="reference internal" href="../modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><tt class="xref py py-class docutils literal"><span class="pre">sklearn.linear_model.SGDClassifier</span></tt></a>.
This is not easily possible for the case of the kernelized SVM.</p>
<p>The second plot visualized the decision surfaces of the RBF kernel SVM and
the linear SVM with approximate kernel map.
The plot shows decision surfaces of the classifiers projected onto
the first two principal components of the data. This visualization should
be taken with a grain of salt since it is just an interesting slice through
the decision surface in 64 dimensions. In particular note that
a datapoint (represented as a dot) does not necessarily be classified
into the region it is lying in, since it will not lie on the plane
that the first two principal components span.</p>
<p>The usage of <a class="reference internal" href="../modules/generated/sklearn.kernel_approximation.RBFSampler.html#sklearn.kernel_approximation.RBFSampler" title="sklearn.kernel_approximation.RBFSampler"><tt class="xref py py-class docutils literal"><span class="pre">RBFSampler</span></tt></a> is described in detail in
<a class="reference internal" href="../modules/kernel_approximation.html#kernel-approximation"><em>Kernel Approximation</em></a>.</p>
<ul class="horizontal">
<li><a class="first reference internal image-reference" href="../_images/plot_kernel_approximation_2.png"><img alt="../_images/plot_kernel_approximation_2.png" src="../_images/plot_kernel_approximation_2.png" style="width: 600.0px; height: 250.0px;" /></a>
</li>
<li><a class="first reference internal image-reference" href="../_images/plot_kernel_approximation_1.png"><img alt="../_images/plot_kernel_approximation_1.png" src="../_images/plot_kernel_approximation_1.png" style="width: 400.0px; height: 300.0px;" /></a>
</li>
</ul>
<p><strong>Python source code:</strong> <a class="reference download internal" href="../_downloads/plot_kernel_approximation.py"><tt class="xref download docutils literal"><span class="pre">plot_kernel_approximation.py</span></tt></a></p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">print</span> <span class="n">__doc__</span>

<span class="c"># Author: Gael Varoquaux &lt;gael dot varoquaux at normalesup dot org&gt;</span>
<span class="c">#         modified Andreas Mueller</span>
<span class="c"># License: Simplified BSD</span>

<span class="c"># Standard scientific Python imports</span>
<span class="kn">import</span> <span class="nn">pylab</span> <span class="kn">as</span> <span class="nn">pl</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">time</span>

<span class="c"># Import datasets, classifiers and performance metrics</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">svm</span><span class="p">,</span> <span class="n">pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.kernel_approximation</span> <span class="kn">import</span> <span class="n">RBFSampler</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>

<span class="c"># The digits dataset</span>
<span class="n">digits</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_digits</span><span class="p">(</span><span class="n">n_class</span><span class="o">=</span><span class="mi">9</span><span class="p">)</span>

<span class="c"># To apply an classifier on this data, we need to flatten the image, to</span>
<span class="c"># turn the data in a (samples, feature) matrix:</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">digits</span><span class="o">.</span><span class="n">data</span> <span class="o">/</span> <span class="mf">16.</span>
<span class="n">data</span> <span class="o">-=</span> <span class="n">data</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c"># We learn the digits on the first half of the digits</span>
<span class="n">data_train</span><span class="p">,</span> <span class="n">targets_train</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:</span><span class="n">n_samples</span> <span class="o">/</span> <span class="mi">2</span><span class="p">],</span> <span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="p">[:</span><span class="n">n_samples</span> <span class="o">/</span> <span class="mi">2</span><span class="p">]</span>


<span class="c"># Now predict the value of the digit on the second half:</span>
<span class="n">data_test</span><span class="p">,</span> <span class="n">targets_test</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">n_samples</span> <span class="o">/</span> <span class="mi">2</span><span class="p">:],</span> <span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="p">[</span><span class="n">n_samples</span> <span class="o">/</span> <span class="mi">2</span><span class="p">:]</span>
<span class="c">#data_test = scaler.transform(data_test)</span>

<span class="c"># Create a classifier: a support vector classifier</span>
<span class="n">kernel_svm</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">gamma</span><span class="o">=.</span><span class="mi">2</span><span class="p">)</span>
<span class="n">linear_svm</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">LinearSVC</span><span class="p">()</span>

<span class="c"># create pipeline from kernel approximation</span>
<span class="c"># and linear svm</span>
<span class="n">feature_map</span> <span class="o">=</span> <span class="n">RBFSampler</span><span class="p">(</span><span class="n">gamma</span><span class="o">=.</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">approx_kernel_svm</span> <span class="o">=</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">Pipeline</span><span class="p">([(</span><span class="s">&quot;feature_map&quot;</span><span class="p">,</span> <span class="n">feature_map</span><span class="p">),</span>
    <span class="p">(</span><span class="s">&quot;svm&quot;</span><span class="p">,</span> <span class="n">svm</span><span class="o">.</span><span class="n">LinearSVC</span><span class="p">())])</span>

<span class="c"># fit and predict using linear and kernel svm:</span>

<span class="n">kernel_svm_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>
<span class="n">kernel_svm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data_train</span><span class="p">,</span> <span class="n">targets_train</span><span class="p">)</span>
<span class="n">kernel_svm_score</span> <span class="o">=</span> <span class="n">kernel_svm</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">data_test</span><span class="p">,</span> <span class="n">targets_test</span><span class="p">)</span>
<span class="n">kernel_svm_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">kernel_svm_time</span>

<span class="n">linear_svm_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>
<span class="n">linear_svm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data_train</span><span class="p">,</span> <span class="n">targets_train</span><span class="p">)</span>
<span class="n">linear_svm_score</span> <span class="o">=</span> <span class="n">linear_svm</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">data_test</span><span class="p">,</span> <span class="n">targets_test</span><span class="p">)</span>
<span class="n">linear_svm_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">linear_svm_time</span>

<span class="n">sample_sizes</span> <span class="o">=</span> <span class="mi">50</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">approx_kernel_scores</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">approx_kernel_times</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">D</span> <span class="ow">in</span> <span class="n">sample_sizes</span><span class="p">:</span>
    <span class="n">approx_kernel_svm</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">feature_map__n_components</span><span class="o">=</span><span class="n">D</span><span class="p">)</span>
    <span class="n">approx_kernel_timing</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>
    <span class="n">approx_kernel_svm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data_train</span><span class="p">,</span> <span class="n">targets_train</span><span class="p">)</span>
    <span class="n">approx_kernel_times</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">approx_kernel_timing</span><span class="p">)</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">approx_kernel_svm</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">data_test</span><span class="p">,</span> <span class="n">targets_test</span><span class="p">)</span>
    <span class="n">approx_kernel_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>

<span class="c"># plot the results:</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">211</span><span class="p">)</span>
<span class="c"># second y axis for timeings</span>
<span class="n">timescale</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">212</span><span class="p">)</span>

<span class="n">accuracy</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sample_sizes</span><span class="p">,</span> <span class="n">approx_kernel_scores</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">&quot;approx. kernel&quot;</span><span class="p">)</span>
<span class="n">timescale</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sample_sizes</span><span class="p">,</span> <span class="n">approx_kernel_times</span><span class="p">,</span> <span class="s">&#39;--&#39;</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="s">&#39;approx. kernel&#39;</span><span class="p">)</span>

<span class="c"># horizontal lines for exact rbf and linear kernels:</span>
<span class="n">accuracy</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">sample_sizes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">sample_sizes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]],</span> <span class="p">[</span><span class="n">linear_svm_score</span><span class="p">,</span>
    <span class="n">linear_svm_score</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">&quot;linear svm&quot;</span><span class="p">)</span>
<span class="n">timescale</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">sample_sizes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">sample_sizes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]],</span> <span class="p">[</span><span class="n">linear_svm_time</span><span class="p">,</span>
        <span class="n">linear_svm_time</span><span class="p">],</span> <span class="s">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">&#39;linear svm&#39;</span><span class="p">)</span>

<span class="n">accuracy</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">sample_sizes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">sample_sizes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]],</span> <span class="p">[</span><span class="n">kernel_svm_score</span><span class="p">,</span>
    <span class="n">kernel_svm_score</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">&quot;rbf svm&quot;</span><span class="p">)</span>
<span class="n">timescale</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">sample_sizes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">sample_sizes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]],</span> <span class="p">[</span><span class="n">kernel_svm_time</span><span class="p">,</span>
        <span class="n">kernel_svm_time</span><span class="p">],</span> <span class="s">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">&#39;rbf svm&#39;</span><span class="p">)</span>

<span class="c"># vertical line for dataset dimensionality = 64</span>
<span class="n">accuracy</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.7</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">&quot;n_features&quot;</span><span class="p">)</span>

<span class="c"># legends and labels</span>
<span class="n">accuracy</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">&quot;Classification accuracy&quot;</span><span class="p">)</span>
<span class="n">timescale</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">&quot;Training times&quot;</span><span class="p">)</span>
<span class="n">accuracy</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="n">sample_sizes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">sample_sizes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="n">accuracy</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(())</span>
<span class="n">accuracy</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">approx_kernel_scores</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">timescale</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">&quot;Sampling steps = transformed feature dimension&quot;</span><span class="p">)</span>
<span class="n">accuracy</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">&quot;Classification accuracy&quot;</span><span class="p">)</span>
<span class="n">timescale</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">&quot;Training time in seconds&quot;</span><span class="p">)</span>
<span class="n">accuracy</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s">&#39;best&#39;</span><span class="p">)</span>
<span class="n">timescale</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s">&#39;best&#39;</span><span class="p">)</span>

<span class="c"># visualize the decision surface, projected down to the first</span>
<span class="c"># two principal components of the dataset</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data_train</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">data_train</span><span class="p">)</span>

<span class="c"># Gemerate grid along first two principal components</span>
<span class="n">multiples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="c"># steps along first component</span>
<span class="n">first</span> <span class="o">=</span> <span class="n">multiples</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span> <span class="o">*</span> <span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span>
<span class="c"># steps along second component</span>
<span class="n">second</span> <span class="o">=</span> <span class="n">multiples</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span> <span class="o">*</span> <span class="n">pca</span><span class="o">.</span><span class="n">components_</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>
<span class="c"># combine</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">first</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">+</span> <span class="n">second</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:]</span>
<span class="n">flat_grid</span> <span class="o">=</span> <span class="n">grid</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="c"># title for the plots</span>
<span class="n">titles</span> <span class="o">=</span> <span class="p">[</span><span class="s">&#39;SVC with rbf kernel&#39;</span><span class="p">,</span>
          <span class="s">&#39;SVC (linear kernel) with rbf feature map</span><span class="se">\n</span><span class="s"> n_components=100&#39;</span><span class="p">]</span>

<span class="n">pl</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">pl</span><span class="o">.</span><span class="n">set_cmap</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Paired</span><span class="p">)</span>


<span class="c"># predict and plot</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">clf</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">((</span><span class="n">kernel_svm</span><span class="p">,</span> <span class="n">approx_kernel_svm</span><span class="p">)):</span>
    <span class="c"># Plot the decision boundary. For that, we will asign a color to each</span>
    <span class="c"># point in the mesh [x_min, m_max]x[y_min, y_max].</span>
    <span class="n">pl</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">flat_grid</span><span class="p">)</span>

    <span class="c"># Put the result into a color plot</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">grid</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">pl</span><span class="o">.</span><span class="n">set_cmap</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Paired</span><span class="p">)</span>
    <span class="n">pl</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">multiples</span><span class="p">,</span> <span class="n">multiples</span><span class="p">,</span> <span class="n">Z</span><span class="p">)</span>
    <span class="n">pl</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s">&#39;off&#39;</span><span class="p">)</span>

    <span class="c"># Plot also the training points</span>
    <span class="n">pl</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">targets_train</span><span class="p">)</span>

    <span class="n">pl</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">titles</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
<span class="n">pl</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>


          </div>
        </div>
      </div>
        <div class="clearer"></div>
      </div>
    </div>

    <div class="footer">
        &copy; 2010â2011, scikit-learn developers (BSD License).
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.1.2. Design by <a href="http://webylimonada.com">Web y Limonada</a>.
    <span style="padding-left: 5ex;">
    <a href="../_sources/auto_examples/plot_kernel_approximation.txt"
	    rel="nofollow">Show this page source</a>
    </span>
    </div>
  </body>
</html>